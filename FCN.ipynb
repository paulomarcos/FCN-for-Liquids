{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import glob\n",
    "from glob import glob\n",
    "import scipy.misc as misc\n",
    "import warnings\n",
    "from distutils.version import LooseVersion\n",
    "import helper\n",
    "import project_tests as tests\n",
    "import skimage\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "  warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "  print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = './data'\n",
    "RUNS_DIRECTORY = './runs'\n",
    "VGG_PATH = './data/vgg'\n",
    "TRAINING_DATA_DIRECTORY ='./data/data_liquid/training'\n",
    "TESTING_DATA_DIRECTORY = './data/data_liquid/testing'\n",
    "\n",
    "dirs = [x[0] for x in os.walk(TESTING_DATA_DIRECTORY)]\n",
    "TESTING_DIRECTORIES = dirs[1:]\n",
    "\n",
    "dirs = [x[0] for x in os.walk(TRAINING_DATA_DIRECTORY)]\n",
    "TRAINING_DIRECTORIES = dirs[1:]\n",
    "\n",
    "# Get the number of images altogether\n",
    "x = 0\n",
    "for d in dirs:\n",
    "    ip = glob(os.path.join(d, 'data*.png'))\n",
    "    x += len(ip)    \n",
    "NUMBER_OF_IMAGES = x # 11192 #len(glob.glob('./data/data_liquid/testing/image_2/*.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 2\n",
    "IMAGE_SHAPE = (480, 640)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "DROPOUT = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_label = tf.placeholder(tf.float32, [None, IMAGE_SHAPE[0], IMAGE_SHAPE[1], NUMBER_OF_CLASSES])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for plotting to visualize if our training is going well given parameters\n",
    "all_training_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "  \"\"\"\n",
    "  Load Pretrained VGG Model into TensorFlow.\n",
    "  sess: TensorFlow Session\n",
    "  vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
    "  return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3, layer4, layer7)\n",
    "  \"\"\"\n",
    "  # load the model and weights\n",
    "  model = tf.saved_model.loader.load(sess, ['vgg16'], vgg_path)\n",
    "\n",
    "  # Get Tensors to be returned from graph\n",
    "  graph = tf.get_default_graph()\n",
    "  image_input = graph.get_tensor_by_name('image_input:0')\n",
    "  keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "  layer3 = graph.get_tensor_by_name('layer3_out:0')\n",
    "  layer4 = graph.get_tensor_by_name('layer4_out:0')\n",
    "  layer7 = graph.get_tensor_by_name('layer7_out:0')\n",
    "\n",
    "  return image_input, keep_prob, layer3, layer4, layer7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1x1(layer, layer_name):\n",
    "  \"\"\" Return the output of a 1x1 convolution of a layer \"\"\"\n",
    "  return tf.layers.conv2d(inputs = layer,\n",
    "                          filters =  NUMBER_OF_CLASSES,\n",
    "                          kernel_size = (1, 1),\n",
    "                          strides = (1, 1),\n",
    "                          name = layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(layer, k, s, layer_name):\n",
    "  \"\"\" Return the output of transpose convolution given kernel_size k and strides s \"\"\"\n",
    "  # See: http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic\n",
    "  return tf.layers.conv2d_transpose(inputs = layer,\n",
    "                                    filters = NUMBER_OF_CLASSES,\n",
    "                                    kernel_size = (k, k),\n",
    "                                    strides = (s, s),\n",
    "                                    padding = 'same',\n",
    "                                    name = layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes = NUMBER_OF_CLASSES):\n",
    "  \"\"\"\n",
    "  Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
    "  vgg_layerX_out: TF Tensor for VGG Layer X output\n",
    "  num_classes: Number of classes to classify\n",
    "  return: The Tensor for the last layer of output\n",
    "  \"\"\"\n",
    "\n",
    "  # Use a shorter variable name for simplicity\n",
    "  layer3, layer4, layer7 = vgg_layer3_out, vgg_layer4_out, vgg_layer7_out\n",
    "\n",
    "  # Apply a 1x1 convolution to encoder layers\n",
    "  layer3x = conv_1x1(layer = layer3, layer_name = \"layer3conv1x1\")\n",
    "  layer4x = conv_1x1(layer = layer4, layer_name = \"layer4conv1x1\")\n",
    "  layer7x = conv_1x1(layer = layer7, layer_name = \"layer7conv1x1\")\n",
    " \n",
    "  # Add decoder layers to the network with skip connections and upsampling\n",
    "  # Note: the kernel size and strides are the same as the example in Udacity Lectures\n",
    "  #       Semantic Segmentation Scene Understanding Lesson 10-9: FCN-8 - Decoder\n",
    "  decoderlayer1 = upsample(layer = layer7x, k = 4, s = 2, layer_name = \"decoderlayer1\")\n",
    "  decoderlayer2 = tf.add(decoderlayer1, layer4x, name = \"decoderlayer2\")\n",
    "  decoderlayer3 = upsample(layer = decoderlayer2, k = 4, s = 2, layer_name = \"decoderlayer3\")\n",
    "  decoderlayer4 = tf.add(decoderlayer3, layer3x, name = \"decoderlayer4\")\n",
    "  decoderlayer_output = upsample(layer = decoderlayer4, k = 16, s = 8, layer_name = \"decoderlayer_output\")\n",
    "\n",
    "  return decoderlayer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_verbose(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes = NUMBER_OF_CLASSES):\n",
    "\n",
    "  # Use a shorter variable name for simplicity\n",
    "  layer3, layer4, layer7 = vgg_layer3_out, vgg_layer4_out, vgg_layer7_out\n",
    "\n",
    "  # Apply a 1x1 convolution to encoder layers\n",
    "  layer3x = conv_1x1(layer = layer3, layer_name = \"layer3conv1x1\")\n",
    "  layer4x = conv_1x1(layer = layer4, layer_name = \"layer4conv1x1\")\n",
    "  layer7x = conv_1x1(layer = layer7, layer_name = \"layer7conv1x1\")\n",
    " \n",
    "  decoderlayer1 = upsample(layer = layer7x, k = 4, s = 2, layer_name = \"decoderlayer1\")\n",
    "  decoderlayer2 = tf.add(decoderlayer1, layer4x, name = \"decoderlayer2\")\n",
    "  decoderlayer3 = upsample(layer = decoderlayer2, k = 4, s = 2, layer_name = \"decoderlayer3\")\n",
    "  decoderlayer4 = tf.add(decoderlayer3, layer3x, name = \"decoderlayer4\")\n",
    "  decoderlayer_output = upsample(layer = decoderlayer4, k = 16, s = 8, layer_name = \"decoderlayer_output\")\n",
    "\n",
    "  return layer3, layer4, layer7, layer3x, layer4x, layer7x, \\\n",
    "         decoderlayer1, decoderlayer2, decoderlayer3, decoderlayer4, decoderlayer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes = NUMBER_OF_CLASSES):\n",
    "  \"\"\"\n",
    "  Build the TensorFLow loss and optimizer operations.\n",
    "  nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "  correct_label: TF Placeholder for the correct label image\n",
    "  learning_rate: TF Placeholder for the learning rate\n",
    "  num_classes: Number of classes to classify\n",
    "  return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "  \"\"\"\n",
    "  # reshape 4D tensors to 2D\n",
    "  # Each row represents a pixel, each column a class\n",
    "  logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "  class_labels = tf.reshape(correct_label, (-1, num_classes))\n",
    "\n",
    "  # The cross_entropy_loss is the cost which we are trying to minimize to yield higher accuracy\n",
    "  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = class_labels)\n",
    "  cross_entropy_loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "  # The model implements this operation to find the weights/parameters that would yield correct pixel labels\n",
    "  train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "\n",
    "  return logits, train_op, cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op,\n",
    "             cross_entropy_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate, saver):\n",
    "  \"\"\"\n",
    "  Train neural network and print out the loss during training.\n",
    "  sess: TF Session\n",
    "  epochs: Number of epochs\n",
    "  batch_size: Batch size\n",
    "  get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "  train_op: TF Operation to train the neural network\n",
    "  cross_entropy_loss: TF Tensor for the amount of loss\n",
    "  input_image: TF Placeholder for input images\n",
    "  correct_label: TF Placeholder for label images\n",
    "  keep_prob: TF Placeholder for dropout keep probability\n",
    "  learning_rate: TF Placeholder for learning rate\n",
    "  \"\"\"\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    \n",
    "    losses, i = [], 0\n",
    "    \n",
    "    for images, labels in get_batches_fn(BATCH_SIZE):\n",
    "        \n",
    "      i += 1\n",
    "    \n",
    "      feed = { input_image: images,\n",
    "               correct_label: labels,\n",
    "               keep_prob: DROPOUT,\n",
    "               learning_rate: LEARNING_RATE }\n",
    "        \n",
    "      _, partial_loss = sess.run([train_op, cross_entropy_loss], feed_dict = feed)\n",
    "      \n",
    "      print(\"---> iteration: \", i, \" partial loss:\", partial_loss)\n",
    "      losses.append(partial_loss)\n",
    "      \n",
    "      if i % 100 == 0:\n",
    "        # Save model\n",
    "        saver.save(sess, \"./model.ckpt\")  \n",
    "          \n",
    "    training_loss = sum(losses) / len(losses)\n",
    "    all_training_losses.append(training_loss)\n",
    "    \n",
    "    print(\"------------------\")\n",
    "    print(\"epoch: \", epoch + 1, \" of \", EPOCHS, \"training loss: \", training_loss)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "  tests.test_layers(layers)\n",
    "  tests.test_optimize(optimize)\n",
    "  tests.test_for_kitti_dataset(DATA_DIRECTORY)\n",
    "  tests.test_train_nn(train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "  print(\"NUMBER OF IMAGES:\", NUMBER_OF_IMAGES)\n",
    "\n",
    "  # download vgg model\n",
    "  helper.maybe_download_pretrained_vgg(DATA_DIRECTORY)\n",
    "\n",
    "  # A function to get batches\n",
    "  get_batches_fn = helper.gen_batch_function(TRAINING_DIRECTORIES, IMAGE_SHAPE)\n",
    "\n",
    " \n",
    "  \n",
    "  with tf.Session() as session:\n",
    "        \n",
    "    # Returns the three layers, keep probability and input layer from the vgg architecture\n",
    "    image_input, keep_prob, layer3, layer4, layer7 = load_vgg(session, VGG_PATH)\n",
    "\n",
    "    # The resulting network architecture, adding a decoder on top of the given vgg model\n",
    "    model_output = layers(layer3, layer4, layer7, NUMBER_OF_CLASSES)\n",
    "\n",
    "    # Returns the output logits, training operation and cost operation to be used\n",
    "    # For the logits: each row represents a pixel, each column a class\n",
    "    # training operation is what is used to get the right parameters to the model to correctly label the pixels\n",
    "    # the cross entropy loss is the cost which we are minimizing, lower cost should yield higher accuracy\n",
    "    logits, train_op, cross_entropy_loss = optimize(model_output, correct_label, learning_rate, NUMBER_OF_CLASSES)\n",
    "    \n",
    "    # Initilize all variables\n",
    "    session.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "    \n",
    "    # Create saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # train the neural network\n",
    "    train_nn(session, EPOCHS, BATCH_SIZE, get_batches_fn, \n",
    "             train_op, cross_entropy_loss, image_input,\n",
    "             correct_label, keep_prob, learning_rate, saver)\n",
    "    \n",
    "    # Save inference data\n",
    "    helper.save_inference_samples(RUNS_DIRECTORY, TESTING_DIRECTORIES, session, IMAGE_SHAPE, logits, keep_prob, image_input)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_shapes():\n",
    "  with tf.Session() as sess:\n",
    "    x = np.random.randn(1, 160, 576, 3)\n",
    "    \n",
    "    image_input, keep_prob, layer3, layer4, layer7 = load_vgg(sess, VGG_PATH)\n",
    " \n",
    "    op = layers_verbose(layer3, layer4, layer7, NUMBER_OF_CLASSES)\n",
    "  \n",
    "    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "\n",
    "    l3, l4, l7, l3x, l4x, l7x, d1, s2, d3, s4, d5 = sess.run(op, feed_dict = {image_input: x, keep_prob: 1.0})\n",
    "\n",
    "    print(\"------------------\")\n",
    "    print(\"shapes of layers:\") \n",
    "    print(\"------------------\")\n",
    "\n",
    "    print(\"layer3 -->\", l3.shape)\n",
    "    print(\"layer4 -->\", l4.shape)\n",
    "    print(\"layer7 -->\", l7.shape)\n",
    "    print(\"layer3 conv1x1 -->\", l3x.shape)\n",
    "    print(\"layer4 conv1x1 -->\", l4x.shape)\n",
    "    print(\"layer7 conv1x1-->\", l7x.shape)\n",
    "    print(\"decoderlayer1 transpose: layer7 k = 4 s = 2 -->\", d1.shape)\n",
    "    print(\"decoderlayer2 skip: decoderlayer1 and layer4conv1x1 -->\", s2.shape)\n",
    "    print(\"decoderlayer3 transpose: decoderlayer2 k = 4 s = 2 -->\", d3.shape)\n",
    "    print(\"decoderlayer4 skip: decoderlayer3 and layer3conv1x1 -->\", s4.shape)\n",
    "    print(\"decoderlayer5 transpose: decoderlayer4 k = 16 s = 8 -->\", d5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF IMAGES: 1521\n",
      "INFO:tensorflow:Restoring parameters from b'./data/vgg/variables/variables'\n",
      "WARNING:tensorflow:From <ipython-input-13-b8179cf1b687>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "---> iteration:  1  partial loss: 51.781162\n",
      "---> iteration:  2  partial loss: 35.443176\n",
      "---> iteration:  3  partial loss: 30.524286\n",
      "---> iteration:  4  partial loss: 23.063112\n",
      "---> iteration:  5  partial loss: 17.825687\n",
      "---> iteration:  6  partial loss: 15.020732\n",
      "---> iteration:  7  partial loss: 13.624409\n",
      "---> iteration:  8  partial loss: 11.92215\n",
      "---> iteration:  9  partial loss: 10.39645\n",
      "---> iteration:  10  partial loss: 9.006702\n",
      "---> iteration:  11  partial loss: 8.062034\n",
      "---> iteration:  12  partial loss: 7.5113297\n",
      "---> iteration:  13  partial loss: 6.878418\n",
      "---> iteration:  14  partial loss: 6.1320734\n",
      "---> iteration:  15  partial loss: 5.4820724\n",
      "---> iteration:  16  partial loss: 4.8426886\n",
      "---> iteration:  17  partial loss: 4.5750327\n",
      "---> iteration:  18  partial loss: 4.3306556\n",
      "---> iteration:  19  partial loss: 3.9448216\n",
      "---> iteration:  20  partial loss: 3.5253673\n",
      "---> iteration:  21  partial loss: 3.309423\n",
      "---> iteration:  22  partial loss: 3.157069\n",
      "---> iteration:  23  partial loss: 3.0239902\n",
      "---> iteration:  24  partial loss: 2.8430855\n",
      "---> iteration:  25  partial loss: 2.6659179\n",
      "---> iteration:  26  partial loss: 2.5703123\n",
      "---> iteration:  27  partial loss: 2.461459\n",
      "---> iteration:  28  partial loss: 2.3320007\n",
      "---> iteration:  29  partial loss: 2.2152314\n",
      "---> iteration:  30  partial loss: 2.1247756\n",
      "---> iteration:  31  partial loss: 2.0632868\n",
      "---> iteration:  32  partial loss: 1.9851086\n",
      "---> iteration:  33  partial loss: 1.9166992\n",
      "---> iteration:  34  partial loss: 1.8472538\n",
      "---> iteration:  35  partial loss: 1.8009511\n",
      "---> iteration:  36  partial loss: 1.7439017\n",
      "---> iteration:  37  partial loss: 1.6710961\n",
      "---> iteration:  38  partial loss: 1.6204879\n",
      "---> iteration:  39  partial loss: 1.5806836\n",
      "---> iteration:  40  partial loss: 1.5471032\n",
      "---> iteration:  41  partial loss: 1.4966712\n",
      "---> iteration:  42  partial loss: 1.4649247\n",
      "---> iteration:  43  partial loss: 1.4305464\n",
      "---> iteration:  44  partial loss: 1.3996524\n",
      "---> iteration:  45  partial loss: 1.369907\n",
      "---> iteration:  46  partial loss: 1.3433133\n",
      "---> iteration:  47  partial loss: 1.3188232\n",
      "---> iteration:  48  partial loss: 1.28863\n",
      "---> iteration:  49  partial loss: 1.2712344\n",
      "---> iteration:  50  partial loss: 1.2519377\n",
      "---> iteration:  51  partial loss: 1.2184724\n",
      "---> iteration:  52  partial loss: 1.2596846\n",
      "---> iteration:  53  partial loss: 1.2397678\n",
      "---> iteration:  54  partial loss: 1.214184\n",
      "---> iteration:  55  partial loss: 1.2001097\n",
      "---> iteration:  56  partial loss: 1.1899554\n",
      "---> iteration:  57  partial loss: 1.1701618\n",
      "---> iteration:  58  partial loss: 1.1539762\n",
      "---> iteration:  59  partial loss: 1.1300799\n",
      "---> iteration:  60  partial loss: 1.122024\n",
      "---> iteration:  61  partial loss: 1.1045265\n",
      "---> iteration:  62  partial loss: 1.08823\n",
      "---> iteration:  63  partial loss: 1.078894\n",
      "---> iteration:  64  partial loss: 1.0678021\n",
      "---> iteration:  65  partial loss: 1.0510558\n",
      "---> iteration:  66  partial loss: 1.0399699\n",
      "---> iteration:  67  partial loss: 1.0297846\n",
      "---> iteration:  68  partial loss: 1.0193853\n",
      "---> iteration:  69  partial loss: 1.0099515\n",
      "---> iteration:  70  partial loss: 0.999632\n",
      "---> iteration:  71  partial loss: 0.9911702\n",
      "---> iteration:  72  partial loss: 0.9820167\n",
      "---> iteration:  73  partial loss: 0.9724235\n",
      "---> iteration:  74  partial loss: 0.9625846\n",
      "---> iteration:  75  partial loss: 0.95703274\n",
      "---> iteration:  76  partial loss: 0.9479312\n",
      "---> iteration:  77  partial loss: 0.94267344\n",
      "---> iteration:  78  partial loss: 0.9333807\n",
      "---> iteration:  79  partial loss: 0.92501676\n",
      "---> iteration:  80  partial loss: 0.9236873\n",
      "---> iteration:  81  partial loss: 0.91697526\n",
      "---> iteration:  82  partial loss: 0.90878296\n",
      "---> iteration:  83  partial loss: 0.9081429\n",
      "---> iteration:  84  partial loss: 0.9004089\n",
      "---> iteration:  85  partial loss: 0.8952242\n",
      "---> iteration:  86  partial loss: 0.8903798\n",
      "---> iteration:  87  partial loss: 0.8838697\n",
      "---> iteration:  88  partial loss: 0.8804965\n",
      "---> iteration:  89  partial loss: 0.87327605\n",
      "---> iteration:  90  partial loss: 0.86835605\n",
      "---> iteration:  91  partial loss: 0.86498106\n",
      "---> iteration:  92  partial loss: 0.8571224\n",
      "---> iteration:  93  partial loss: 0.8569842\n",
      "---> iteration:  94  partial loss: 0.8554747\n",
      "---> iteration:  95  partial loss: 0.8454274\n",
      "---> iteration:  96  partial loss: 0.84334856\n",
      "---> iteration:  97  partial loss: 0.83861816\n",
      "---> iteration:  98  partial loss: 0.83754736\n",
      "---> iteration:  99  partial loss: 0.8349603\n",
      "---> iteration:  100  partial loss: 0.8332767\n",
      "---> iteration:  101  partial loss: 0.8285144\n",
      "---> iteration:  102  partial loss: 0.8251704\n",
      "---> iteration:  103  partial loss: 0.82435745\n",
      "---> iteration:  104  partial loss: 0.82083637\n",
      "---> iteration:  105  partial loss: 0.821614\n",
      "---> iteration:  106  partial loss: 0.8109965\n",
      "---> iteration:  107  partial loss: 0.8052431\n",
      "---> iteration:  108  partial loss: 0.80812323\n",
      "---> iteration:  109  partial loss: 0.80225194\n",
      "---> iteration:  110  partial loss: 0.7976792\n",
      "---> iteration:  111  partial loss: 0.79544044\n",
      "---> iteration:  112  partial loss: 0.79242575\n",
      "---> iteration:  113  partial loss: 0.78814024\n",
      "---> iteration:  114  partial loss: 0.785459\n",
      "---> iteration:  115  partial loss: 0.7806312\n",
      "---> iteration:  116  partial loss: 0.7788841\n",
      "---> iteration:  117  partial loss: 0.7728993\n",
      "---> iteration:  118  partial loss: 0.77382004\n",
      "---> iteration:  119  partial loss: 0.76970005\n",
      "---> iteration:  120  partial loss: 0.76667166\n",
      "---> iteration:  121  partial loss: 0.7612822\n",
      "---> iteration:  122  partial loss: 0.7586997\n",
      "---> iteration:  123  partial loss: 0.76048225\n",
      "---> iteration:  124  partial loss: 0.7552753\n",
      "---> iteration:  125  partial loss: 0.7531845\n",
      "---> iteration:  126  partial loss: 0.74984014\n",
      "---> iteration:  127  partial loss: 0.7493141\n",
      "---> iteration:  128  partial loss: 0.7459747\n",
      "---> iteration:  129  partial loss: 0.7430542\n",
      "---> iteration:  130  partial loss: 0.7406756\n",
      "---> iteration:  131  partial loss: 0.74048334\n",
      "---> iteration:  132  partial loss: 0.73662233\n",
      "---> iteration:  133  partial loss: 0.7357806\n",
      "---> iteration:  134  partial loss: 0.73625124\n",
      "---> iteration:  135  partial loss: 0.73089224\n",
      "---> iteration:  136  partial loss: 0.73085594\n",
      "---> iteration:  137  partial loss: 0.72737974\n",
      "---> iteration:  138  partial loss: 0.7272824\n",
      "---> iteration:  139  partial loss: 0.7234009\n",
      "---> iteration:  140  partial loss: 0.7212135\n",
      "---> iteration:  141  partial loss: 0.72194076\n",
      "---> iteration:  142  partial loss: 0.71728605\n",
      "---> iteration:  143  partial loss: 0.71657085\n",
      "---> iteration:  144  partial loss: 0.71323746\n",
      "---> iteration:  145  partial loss: 0.71218777\n",
      "---> iteration:  146  partial loss: 0.71227956\n",
      "---> iteration:  147  partial loss: 0.71120435\n",
      "---> iteration:  148  partial loss: 0.7114026\n",
      "---> iteration:  149  partial loss: 0.70690894\n",
      "---> iteration:  150  partial loss: 0.70783234\n",
      "---> iteration:  151  partial loss: 0.70546156\n",
      "---> iteration:  152  partial loss: 0.701758\n",
      "---> iteration:  153  partial loss: 0.7006782\n",
      "------------------\n",
      "epoch:  1  of  1 training loss:  2.7538732453888537\n",
      "------------------\n",
      "Training Finished. Saving test images to: ./runs/1538644322.4098666\n",
      "@@@@@@@@@@IMAGE OUTPUTS@@@@@@@\n",
      "./data/data_liquid/testing/Copy_of_scene_left_fruitBowl_mug_empty_dump_high\n",
      "./data/data_liquid/testing/Copy_of_scene_left_fruitBowl_mug_90%_hold_moderate\n",
      "./data/data_liquid/testing/Copy_of_scene_left_fruitBowl_mug_60%_hold_high\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.7538732453888537]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Graph constructed\n",
      "Variables initialized\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gother/.local/lib/python3.5/site-packages/ipykernel_launcher.py:31: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/data_liquid/testing/image_2/data0257.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-52fe17759d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# TEST IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/data_liquid/testing/image_2/data0257.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SHAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(name, flatten, mode)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \"\"\"\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/data_liquid/testing/image_2/data0257.png'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    # Restore variables and model\n",
    "    saver = tf.train.import_meta_graph(\"./model.ckpt.meta\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "   \n",
    "    # Get the three layers, keep probability and input layer\n",
    "    graph = tf.get_default_graph()\n",
    "    image_input = graph.get_tensor_by_name('image_input:0')\n",
    "    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "    layer3 = graph.get_tensor_by_name('layer3_out:0')\n",
    "    layer4 = graph.get_tensor_by_name('layer4_out:0')\n",
    "    layer7 = graph.get_tensor_by_name('layer7_out:0')\n",
    "\n",
    "    # The resulting network architecture, adding a decoder on top of the given vgg model\n",
    "    model_output = layers(layer3, layer4, layer7, NUMBER_OF_CLASSES)\n",
    "\n",
    "    logits = tf.reshape(model_output, (-1, NUMBER_OF_CLASSES))\n",
    "    print('Graph constructed')\n",
    "    \n",
    "    # Initilize all variables\n",
    "    session.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "    print('Variables initialized')\n",
    "    \n",
    "    # Restore variables that were trained from previous session\n",
    "    saver.restore(session, tf.train.latest_checkpoint(\"./\"))\n",
    "    \n",
    "    # TEST IMAGE\n",
    "    test_image = scipy.misc.imresize(scipy.misc.imread(\"./data/data_liquid/testing/image_2/data0257.png\"), IMAGE_SHAPE)\n",
    "    \n",
    "    # Make predictions\n",
    "    feed = {keep_prob: 1.0, image_input: [test_image]}\n",
    "    im_softmax = session.run( [tf.nn.softmax(logits)], feed)\n",
    "    im_softmax = im_softmax[0][:, 1].reshape(IMAGE_SHAPE[0], IMAGE_SHAPE[1])\n",
    "    segmentation = (im_softmax > 0.5).reshape(IMAGE_SHAPE[0], IMAGE_SHAPE[1], 1)\n",
    "    mask = np.dot(segmentation, np.array([[0, 255, 0, 127]]))\n",
    "    mask = scipy.misc.toimage(mask, mode=\"RGBA\")\n",
    "    result = scipy.misc.toimage(test_image)\n",
    "    result.paste(mask, box=None, mask=mask)\n",
    "    plt.imshow(result)\n",
    "    plt.show()  \n",
    "   \n",
    "    \n",
    "    #helper.save_inference_samples(RUNS_DIRECTORY, DATA_DIRECTORY, session, IMAGE_SHAPE, logits, keep_prob, image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "  print(\"NUMBER OF IMAGES:\", NUMBER_OF_IMAGES)\n",
    "\n",
    "  # download vgg model\n",
    "  helper.maybe_download_pretrained_vgg(DATA_DIRECTORY)\n",
    "\n",
    "  # A function to get batches\n",
    "  get_batches_fn = helper.gen_batch_function(TRAINING_DATA_DIRECTORY, IMAGE_SHAPE)\n",
    "  tf.reset_default_graph()\n",
    "   \n",
    "  with tf.Session() as session:\n",
    "    \n",
    "    # Restore variables and model\n",
    "    saver = tf.train.import_meta_graph(\"./fcn_liquid_model-500.meta\")\n",
    "    saver.restore(session, tf.train.latest_checkpoint(\"./\"))\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "     # Initilize all variables\n",
    "    session.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "        \n",
    "    # load the model and weights\n",
    "    model = tf.saved_model.loader.load(session, ['vgg16'], VGG_PATH)\n",
    "\n",
    "    # Get Tensors to be returned from graph\n",
    "    graph = tf.get_default_graph()\n",
    "    image_input = graph.get_tensor_by_name('image_input:0')\n",
    "    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "    layer3 = graph.get_tensor_by_name('layer3_out:0')\n",
    "    layer4 = graph.get_tensor_by_name('layer4_out:0')\n",
    "    layer7 = graph.get_tensor_by_name('layer7_out:0')\n",
    "    class_labels = tf.reshape(correct_label, (-1, NUMBER_OF_CLASSES))\n",
    "\n",
    "    # The resulting network architecture, adding a decoder on top of the given vgg model\n",
    "    model_output = layers(layer3, layer4, layer7, NUMBER_OF_CLASSES)\n",
    "    \n",
    "    # reshape 4D tensors to 2D\n",
    "    # Each row represents a pixel, each column a class\n",
    "    logits = tf.reshape(model_output, (-1, NUMBER_OF_CLASSES))\n",
    "   \n",
    "\n",
    "    # The cross_entropy_loss is the cost which we are trying to minimize to yield higher accuracy\n",
    "    #cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = class_labels)\n",
    "    #cross_entropy_loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # The model implements this operation to find the weights/parameters that would yield correct pixel labels\n",
    "    #train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)    \n",
    "    \n",
    "    # TEST IMAGE\n",
    "    test_image = scipy.misc.imresize(scipy.misc.imread(\"./data/data_liquid/testing/image_2/data0257.png\"), IMAGE_SHAPE)\n",
    "    \n",
    "    im_softmax = session.run(\n",
    "            [tf.nn.softmax(logits)],\n",
    "            {keep_prob: 1.0, image_input: [test_image]})\n",
    "    \"\"\"\n",
    "    im_softmax = im_softmax[0][:, 1].reshape(IMAGE_SHAPE[0], IMAGE_SHAPE[1])\n",
    "    segmentation = (im_softmax > 0.5).reshape(IMAGE_SHAPE[0], IMAGE_SHAPE[1], 1)\n",
    "    mask = np.dot(segmentation, np.array([[0, 255, 0, 127]]))\n",
    "    mask = scipy.misc.toimage(mask, mode=\"RGBA\")\n",
    "    result = scipy.misc.toimage(test_image)\n",
    "    result.paste(mask, box=None, mask=mask)\n",
    "    plt.imshow(result)\n",
    "    plt.show() \"\"\"\n",
    "   \n",
    "    # Save inference data\n",
    "    #helper.save_inference_samples(RUNS_DIRECTORY, DATA_DIRECTORY, session, IMAGE_SHAPE, logits, keep_prob, image_input)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_traintest():\n",
    "  print(\"NUMBER OF IMAGES:\", NUMBER_OF_IMAGES)\n",
    "\n",
    "  # download vgg model\n",
    "  helper.maybe_download_pretrained_vgg(DATA_DIRECTORY)\n",
    "\n",
    "  # A function to get batches\n",
    "  get_batches_fn = helper.gen_batch_function(TRAINING_DIRECTORIES, IMAGE_SHAPE)\n",
    "\n",
    " \n",
    "  \n",
    "  with tf.Session() as session:\n",
    "        \n",
    "    # Returns the three layers, keep probability and input layer from the vgg architecture\n",
    "    image_input, keep_prob, layer3, layer4, layer7 = load_vgg(session, VGG_PATH)\n",
    "\n",
    "    # The resulting network architecture, adding a decoder on top of the given vgg model\n",
    "    model_output = layers(layer3, layer4, layer7, NUMBER_OF_CLASSES)\n",
    "\n",
    "    # Returns the output logits, training operation and cost operation to be used\n",
    "    # For the logits: each row represents a pixel, each column a class\n",
    "    # training operation is what is used to get the right parameters to the model to correctly label the pixels\n",
    "    # the cross entropy loss is the cost which we are minimizing, lower cost should yield higher accuracy\n",
    "    logits, train_op, cross_entropy_loss = optimize(model_output, correct_label, learning_rate, NUMBER_OF_CLASSES)\n",
    "    \n",
    "    # Create saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Initilize all variables\n",
    "    session.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "\n",
    "    # train the neural network\n",
    "    train_nn(session, EPOCHS, BATCH_SIZE, get_batches_fn, \n",
    "             train_op, cross_entropy_loss, image_input,\n",
    "             correct_label, keep_prob, learning_rate)\n",
    "    \n",
    "    # Save inference data\n",
    "    helper.save_inference_samples(RUNS_DIRECTORY, DATA_DIRECTORY, session, IMAGE_SHAPE, logits, keep_prob, image_input)    \n",
    "    \n",
    "    # Save model\n",
    "    saver.save(session, \"./fcn_liquid_model\", global_step = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # Restore model graph\n",
    "    saver = tf.train.import_meta_graph(\"./model.ckpt.meta\")\n",
    "\n",
    "    # Get the three layers, keep probability and input layer\n",
    "    graph = tf.get_default_graph()\n",
    "    image_input = graph.get_tensor_by_name('image_input:0')\n",
    "    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "    layer3 = graph.get_tensor_by_name('layer3_out:0')\n",
    "    layer4 = graph.get_tensor_by_name('layer4_out:0')\n",
    "    layer7 = graph.get_tensor_by_name('layer7_out:0')\n",
    "\n",
    "    # The resulting network architecture, adding a decoder on top of the given vgg model\n",
    "    model_output = layers(layer3, layer4, layer7, NUMBER_OF_CLASSES)\n",
    "    logits = tf.reshape(model_output, (-1, NUMBER_OF_CLASSES))\n",
    "    print('Graph constructed')\n",
    "\n",
    "\t# Initialize all variables\n",
    "    session.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "    print('Variables initialized')\n",
    "\n",
    "    # Restore variables that were trained from previous session (a subset of all global & local variables).\n",
    "    saver.restore(session, tf.train.latest_checkpoint(\"./\"))\n",
    "    print(\"Trained Variables restored from previous session.\")\n",
    "    \n",
    "    print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    # load image\n",
    "    test_image = scipy.misc.imresize(scipy.misc.imread(\"./data/data_liquid/testing/image_2/data0257.png\"), IMAGE_SHAPE)\n",
    "\n",
    "    # Make predictions\n",
    "    feed = {keep_prob: 1.0, image_input: [test_image]}\n",
    "    im_softmax = session.run( [tf.nn.softmax(logits)], feed)\n",
    "    im_softmax = im_softmax[0][:, 1].reshape(IMAGE_SHAPE[0], IMAGE_SHAPE[1])\n",
    "    segmentation = (im_softmax > 0.5).reshape(IMAGE_SHAPE[0], IMAGE_SHAPE[1], 1)\n",
    "    mask = np.dot(segmentation, np.array([[0, 255, 0, 127]]))\n",
    "    mask = scipy.misc.toimage(mask, mode=\"RGBA\")\n",
    "    result = scipy.misc.toimage(test_image)\n",
    "    result.paste(mask, box=None, mask=mask)\n",
    "    plt.imshow(result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "\n",
    "directory = \"./data/data_liquid/training\"\n",
    "dirs = [x[0] for x in os.walk(directory)]\n",
    "#print(dirs[1:])\n",
    "dirs = dirs[1:]\n",
    "\n",
    "image_paths = glob(os.path.join(TRAINING_DATA_DIRECTORY, 'image_2', '*.png'))\n",
    "#print(image_paths)\n",
    "\n",
    "x = 0\n",
    "for d in dirs:\n",
    "    ip = glob(os.path.join(d, 'data*.png'))\n",
    "    x += len(ip)\n",
    "\n",
    "print(x)\n",
    "#print(ip)\n",
    "\n",
    "if image_paths == ip:\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")\n",
    "\n",
    "label_paths = {\n",
    "                re.sub(r'ground_truth', 'data', os.path.basename(path)): path\n",
    "                for path in glob(os.path.join(TRAINING_DATA_DIRECTORY, 'gt_image_2', '*.png'))}\n",
    "\n",
    "#print(label_paths)\n",
    "\n",
    "\n",
    "lp = {\n",
    "       re.sub(r'ground_truth', 'data', os.path.basename(path)): path\n",
    "       for path in glob(os.path.join(dirs[1], 'ground_truth*.png'))}\n",
    "\n",
    "print(lp)\n",
    "\n",
    "\n",
    "\n",
    "if label_paths == lp:\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
